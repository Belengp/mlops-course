{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üë∑ Serving a Model into Production"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we train a machine learning model using MLflow, we create a powerful tool that can make predictions based on what it has learned from a lot of data. However, to make those predictions accessible and usable in real-world applications, we need a way to serve the model.\n",
    "\n",
    "**Model serving refers to the process of making the trained model available and accessible to other software applications or systems**. It's like setting up a special window through which others can ask questions to the model and get predictions in return.\n",
    "\n",
    "FastAPI is a framework that helps us build web applications and APIs quickly and easily. By combining MLflow with FastAPI, we can create an API endpoint that allows other applications or systems to send data to the model and receive predictions in response.\n",
    "\n",
    "Here's how it works:\n",
    "\n",
    "1. We train and save the machine learning model using MLflow. This model contains all the knowledge it has acquired from the data.\n",
    "\n",
    "2. We use FastAPI to build a web application or API. We create a specific route or endpoint in our application that can receive data from other systems.\n",
    "\n",
    "3. When an external system wants to make a prediction, it sends the data to the API endpoint created with FastAPI.\n",
    "\n",
    "4. FastAPI receives the data and passes it to the MLflow model, which then makes the prediction based on its learned knowledge.\n",
    "\n",
    "5. FastAPI sends the prediction back to the requesting system, which can use the prediction for further processing or display it to the user.\n",
    "\n",
    "In summary, model serving from MLflow into FastAPI allows us to deploy and expose our trained machine learning models as a service. It enables other systems or applications to interact with the model, send data for prediction, and receive the predictions in return. This integration between MLflow and FastAPI opens up a world of possibilities for using machine learning models in real-time applications and systems."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import numpy as np\n",
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "from mlops_course import config"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚¨áÔ∏è Download model from MLFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LogisticRegression from version 1.2.1 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# URI is the URL where the model is stored\n",
    "MODEL_NAME = config.MODEL_NAME\n",
    "MODEL_URI = f\"models:/{MODEL_NAME}/Production\"\n",
    "\n",
    "# Load the MLflow model into memory\n",
    "mlflow.set_tracking_uri(uri=config.MLFLOW_TRACKING_URI)\n",
    "model = mlflow.sklearn.load_model(model_uri=MODEL_URI)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ú® Create the API"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the FastAPI app\n",
    "app = FastAPI()\n",
    "\n",
    "# Define the input data schema using Pydantic BaseModel\n",
    "class InputData(BaseModel):\n",
    "    sex: int\n",
    "    age: float\n",
    "    fare: float\n",
    "\n",
    "\n",
    "# Define the API endpoint\n",
    "@app.post('/predict')\n",
    "def predict(input_data: InputData):\n",
    "    # Process the input data\n",
    "    features = np.array([\n",
    "        input_data.sex,\n",
    "        input_data.age,\n",
    "        input_data.fare,\n",
    "    ]).reshape(1, -1)\n",
    "\n",
    "    # Use the MLflow model to perform inference\n",
    "    prediction = model.predict(features)\n",
    "\n",
    "    # Return the inference results\n",
    "    return {'prediction': prediction.tolist()}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Launch the API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uvicorn\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    config = uvicorn.Config(app)\n",
    "    server = uvicorn.Server(config)\n",
    "    await server.serve()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
